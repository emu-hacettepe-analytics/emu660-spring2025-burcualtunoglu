---
title: "Assignment 1"
---

# Assignment 1

## (a) fgajdgfwkfuhSummary of "Veri Bilimi ve Endüstri Mühendisliği Üzerine Sohbetler - Baykal Hafızoğlu & Erdi Daşdemir"

Data analytics and operations research are related to industrial engineering in four main categories: descriptive, diagnostic, predictive and prescriptive analytics. While artificial intelligence and machine learning have made great advances in the field of predictive analytics in recent years, optimization problems are still dependent on the human factor. Hafızoğlu's career spans a wide range of areas such as supply chain optimization, dynamic pricing and credit scoring systems, emphasizing the need for accurate problem definition, efficient model deployment and user-friendly interfaces for academic models to be effective in the business world. In particular, early prototyping and a continuous feedback loop are recommended to increase the adoption of optimization models.

Although startup ventures in optimization and data science are on the rise in Turkey, investments are limited compared to developed countries. Mathematical modeling and software development skills are critical for those who want to pursue a career in this field. Gaining competence in Python and optimization techniques, deepening analytical skills with a master's or PhD education for those who want to advance in the academic world, and gaining applied technical competencies for those who want to get an early start in professional life will provide a competitive advantage. In conclusion, Hafızoğlu's talk sheds light on the critical relationships between industrial engineering and data science and emphasizes that technical knowledge, application processes and user experience must be integrated for the success of optimization models.

## (b) Exploring Statistical Summaries of ‘mtcars’ dataset

```{r}

data(mtcars)
print(colnames(mtcars))

compute_stats <- function(b) {
  list(
    mean = mean(b, na.rm = TRUE),
    median = median(b, na.rm = TRUE),
    variance = var(b, na.rm = TRUE),
    IQR = IQR(b, na.rm = TRUE),
    min = min(b, na.rm = TRUE),
    max = max(b, na.rm = TRUE)
  )
}


for (col in names(mtcars)) {
  if (is.numeric(mtcars[[col]])) {  
    cat("\nStatistics for:", col, "\n")
    print(compute_stats(mtcars[[col]]))
  }
}


statistics_sapply <- sapply(mtcars, compute_stats)

print("Applying compute_stats Using sapply()")

print(statistics_sapply)

statistics_apply <- apply(mtcars, MARGIN = 2, compute_stats)
print("Applying compute_stats Using apply()")
print(statistics_apply)

```

## (c) Displaying and Manipulating the ‘na_example’ Dataset

```{r}
library(dslabs)
data("na_example")
print(na_example)


na_count <- sum(is.na(na_example))
print(na_count) #Total NA values



na_indic <- which(is.na(na_example))
print(na_indic) #Indices of NA values



before_mean <- mean(na_example, na.rm = TRUE) #mean ignoring NA values
print(before_mean) #Mean before handling NA values

before_sd <- sd(na_example, na.rm = TRUE)  #standard deviation ignoring NA values
print(before_sd) #Standard Deviation before handling NA values




median_value <- median(na_example, na.rm = TRUE) # Compute the median of non-missing values


na_replaced_median <- na_example
na_replaced_median[is.na(na_replaced_median)] <- median_value # Replace NA values with the median


non_na_values <- na_example[!is.na(na_example)]
random_values <- sample(non_na_values, size = sum(is.na(na_example)), replace = TRUE) # Select a random non-missing value


na_replaced_random <- na_example
na_replaced_random[is.na(na_replaced_random)] <- random_values # Replace NA values with random values


# Version 1 (NA replaced with median)
mean_median <- mean(na_replaced_median)  #mean- NA replaced with median
print(mean_median)   #Mean after replacing NA with median
sd_median <- sd(na_replaced_median)  #standard deviation- NA replaced with median
print(sd_median)  # Standard Deviation after replacing NA with median

# Version 2 (NA replaced with random values)
mean_random <- mean(na_replaced_random)
print(mean_random) #Mean after replacing NA with random values
sd_random <- sd(na_replaced_random)
print(sd_random) #Standard Deviation after replacing NA with random values
```

**Comparison of Results**

|                                  | Mean  | Standard Deviation |
|----------------------------------|-------|--------------------|
| Before Handling NAs              | 2.302 | 1.223              |
| Replacing NAs with Median        | 2.258 | 1.136              |
| Replacing NAs with Random Values | 2.304 | 1.223              |

For this dataset, the method of filling missing data with randomly available values seems more appropriate. This is because this method preserves the mean and standard deviation values of the data set, thus minimizing the distortion of the original data structure. The method of filling with the median, on the other hand, reduced the variance of the data set, thus changing the data distribution and causing possible loss of information.
